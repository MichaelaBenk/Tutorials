{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Explaining House Values with Counterfactual Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case study we explain the census tract (median) home values from the Boston metropolitan area with counterfactual explanations using alibi. We will closely follow the exposition in \n",
    "https://docs.seldon.io/projects/alibi/en/stable/examples/cfproto_housing.html\n",
    "to which we refer for all details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started with Python and Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, Jupyter Notebook and Python settings are initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notebook settings\n",
    "###################\n",
    "\n",
    "# resetting variables\n",
    "get_ipython().magic('reset -sf') \n",
    "\n",
    "# formatting: cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by initializing a seed for reproducability. Next we import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'alibi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cc80b9904c81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malibi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounterFactualProto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TF version: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'alibi'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(40) # suppress deprecation messages\n",
    "tf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from sklearn.datasets import load_boston\n",
    "from alibi.explainers import CounterFactualProto\n",
    "\n",
    "print('TF version: ', tf.__version__)\n",
    "print('Eager execution enabled: ', tf.executing_eagerly()) # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "boston = load_boston()\n",
    "data = boston.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target variable\n",
    "feature_names = boston.feature_names\n",
    "target = boston.target\n",
    "print(feature_names)\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical target variable; when higher than median, give =1 \n",
    "y_cat = np.zeros((target.shape[0],))\n",
    "y_cat[np.where(target > np.median(target))[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rapid check on y_cat\n",
    "y_cat[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median of the variable target \n",
    "np.median(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Explorative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data as dataframe - we join the target variables, as well\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df = pd.concat([df, pd.Series(target, name='target')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we describe data\n",
    "pd.set_option('precision', 2)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualizations\n",
    "\n",
    "# histograms\n",
    "df.hist(bins=10,figsize=(15, 15), grid=False)\n",
    "\n",
    "# save the plot \n",
    "#plt.savefig('INSERT PATH')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multivariate exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis\n",
    "pd.set_option('precision', 2)\n",
    "df.drop(['CHAS'], axis=1).corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bivariate plots with high correlation features (>0.4)\n",
    "features = df.drop(['CRIM', 'ZN', 'CHAS',  'AGE', 'DIS', 'RAD', 'B', 'target'], axis = 1)\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# i: index\n",
    "for i, col in enumerate(features.columns):\n",
    "    # 3 plots here hence 1, 3\n",
    "    plt.subplot(1, 6, i+1)\n",
    "    x = df[col]\n",
    "    y = df.target\n",
    "    plt.plot(x, y, 'o')\n",
    "    \n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('target')\n",
    "\n",
    "#plt.savefig('INSERT PATH')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delecte categorical variable CHAS \n",
    "# CHAS = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "data = np.delete(data, 3, 1)\n",
    "feature_names = np.delete(feature_names, 3)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data \n",
    "mu = data.mean(axis=0)\n",
    "sigma = data.std(axis=0)\n",
    "data = (data - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine learning Modeling with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split data into train and test, before categorizing y_train and y_test\n",
    "idx = 475\n",
    "x_train, y_train = data[:idx,:], y_cat[:idx]\n",
    "x_test, y_test = data[idx:,:], y_cat[idx:]\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check on shapes\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute two distinct metrics\n",
    "from tensorflow import keras\n",
    "METRICS = [ \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.AUC(name='auc')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network: model definition\n",
    "def nn_model():\n",
    "    x_in = Input(shape=(12,))\n",
    "    x = Dense(40, activation='relu')(x_in)\n",
    "    x = Dense(40, activation='relu')(x)\n",
    "    x_out = Dense(2, activation='softmax')(x)\n",
    "    nn = Model(inputs=x_in, outputs=x_out)\n",
    "    nn.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=METRICS)\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting and saving the model\n",
    "nn = nn_model()\n",
    "nn.summary()\n",
    "nn.fit(x_train, y_train, batch_size=64, epochs=500, verbose=0)\n",
    "nn.save('INSERT MODEL NAME.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model and computing test performance\n",
    "nn = load_model('INSERT MODEL NAME.h5')  \n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print(nn.metrics_names)\n",
    "print('Test accuracy: ', score[1])\n",
    "print('Test AUC: ', score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model weights\n",
    "nn.save_weights('INSERT PATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generation of Counterfactual Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose a data point (whose outcome) to be explained\n",
    "# in the notes we used x_test[1] and x_test[2]\n",
    "X = x_test[1].reshape((1,) + x_test[1].shape)\n",
    "shape = X.shape\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalizing X - original values\n",
    "orig = X * sigma + mu\n",
    "orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint: use the fitted model or load another one\n",
    "nn = load_model('INSERT MODEL NAME.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alibi imports\n",
    "import alibi\n",
    "from alibi.explainers import CounterFactual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameters for configurations 1,2\n",
    "target_proba = 1.0\n",
    "tol = 0.1\n",
    "target_class = 'other'\n",
    "max_iter = 1000\n",
    "lam_init = 1\n",
    "max_lam_steps = 100\n",
    "learning_rate_init = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation to compute counterfactuals\n",
    "cf = CounterFactual(nn, \n",
    "                    target_proba=target_proba, \n",
    "                    target_class='other',\n",
    "                    tol=tol, \n",
    "                    shape=shape,\n",
    "                    #early_stop=50,\n",
    "                    feature_range= (x_train.min(axis=0), x_train.max(axis=0)), \n",
    "                    max_iter=max_iter, \n",
    "                    lam_init=lam_init, \n",
    "                    max_lam_steps=max_lam_steps,\n",
    "                    learning_rate_init=learning_rate_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing counterfactuals\n",
    "%%time\n",
    "explanation = cf.explain(X)\n",
    "counterfactual = explanation.cf['X'][0]\n",
    "counterfactual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of X\n",
    "explanation.orig_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities of the counterfactual\n",
    "explanation.cf['proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counterfactual data point\n",
    "counterfactual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking classes\n",
    "print('Original prediction: {}'.format(explanation.orig_class))\n",
    "print('Counterfactual prediction: {}'.format(explanation.cf['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of deltas with tolerance 1e-4 (see notes)\n",
    "orig = X * sigma + mu\n",
    "counterfactual = explanation.cf['X'] * sigma + mu\n",
    "delta = counterfactual - orig\n",
    "for i, f in enumerate(feature_names):\n",
    "    if np.abs(delta[0][i]) > 1e-4:\n",
    "        print('{}: {}'.format(f, delta[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eend of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
