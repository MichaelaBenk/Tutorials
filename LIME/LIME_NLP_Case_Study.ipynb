{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Explaining Spam Classification Using LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started with Python and Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, Jupyter Notebook and Python settings are initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook settings\n",
    "###################\n",
    "\n",
    "# resetting variables\n",
    "#get_ipython().magic('reset -sf') \n",
    "\n",
    "# formatting: cell width\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# plotting\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by initializing a seed for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(245)\n",
    "random_state=245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#insert path \n",
    "sys.path.insert(1, 'INSERT PATH')\n",
    "\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the UCI YouTube Spam Collection Data Set, which can be downloaded under the following link: http://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# insert path for CSV file location\n",
    "path = 'INSERT PATH'\n",
    "csvfiles = glob.glob(os.path.join(path, '*.csv'))\n",
    "dataframes = [] \n",
    "\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile, usecols=[\"CONTENT\", \"CLASS\"])\n",
    "    dataframes.append(df)\n",
    "\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "df.columns = ['content', 'class']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Duplicate Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "duplicates = df[df.duplicated()]  #equivalent to keep = first. Duplicated rows, except the first entry, are marked as 'True'\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check\n",
    "df[df.duplicated(subset='content')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a lot of noise. We do not extensively clean the data, but perform a minimum amount of pre-processing, which includes substituting non-alphanumeric characters with whitespaces and normalizing URLs and numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    #changing bad encodings back to apostrophe\n",
    "    text = re.sub('&#39', '\\'', text) \n",
    "    #replacing URL's with placeholder\n",
    "    text = re.sub('http\\S+', 'URL', text) \n",
    "    #replacing URL's with placeholder\n",
    "    text = re.sub('www\\S+', 'URL', text)\n",
    "    #removing HTML markup\n",
    "    text = re.sub('<br */>', ' ', text) \n",
    "    #removing HTML markup\n",
    "    text = re.sub('<[^>]*>', ' ', text) \n",
    "    #replacing digits with placeholder\n",
    "    text = re.sub('\\d+', ' NUM ', text)\n",
    "    #removing remaining noisy characters, smileys, punctuation\n",
    "    text = re.sub('[\\W_]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "df.content = df.content.apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick check\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. A quick check of data structureÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported data structure\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns in data\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double-check first few entries \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Basic linguistic analysis of YouTube comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute word counts per comment, by tokenizing by whitespace\n",
    "df['word_count'] = df['content'].apply(lambda x: len(x.strip().split(\" \")))\n",
    "df[['content','class', 'word_count']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics of word counts\n",
    "print(df['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution of word counts per class\n",
    "from matplotlib import pyplot\n",
    "\n",
    "ham = df[df['class']==0]\n",
    "spam = df[df['class']==1]\n",
    "\n",
    "pyplot.hist(ham.word_count, bins=50, alpha=0.5, label='ham', density=True)\n",
    "pyplot.hist(spam.word_count, bins=50, alpha=0.5, label='spam', density=True)\n",
    "pyplot.xlim((-2,150))\n",
    "pyplot.ylim((0,0.1))\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word counts per each class\n",
    "print (ham['word_count'].describe())\n",
    "print ()\n",
    "print (spam['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word length per comment\n",
    "def avg_word(sentence):\n",
    "    words = sentence.strip().split()\n",
    "    return (sum(len(word) for word in words)/len(words)) if len(words) > 0 else 1\n",
    "\n",
    "\n",
    "df['avg_word'] = df['content'].apply(avg_word)\n",
    "\n",
    "ham = df[df['class']==0]\n",
    "spam = df[df['class']==1]\n",
    "\n",
    "# plotting histograms of average word length per each class\n",
    "pyplot.hist(ham.avg_word, bins=50, alpha=0.5, label='ham', density=True)\n",
    "pyplot.hist(spam.avg_word, bins=50, alpha=0.5, label='spam', density=True)\n",
    "pyplot.xlim((0,15))\n",
    "pyplot.ylim((0,1))\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word length per class: summary statistics\n",
    "print(ham.avg_word.describe())\n",
    "print()\n",
    "print(spam.avg_word.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop word statistics - english stopwords from spacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "df['stopwords'] = df['content'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() in stopwords]))\n",
    "df['stopwords_count'] = df['stopwords'].apply(lambda x: len(x.split()))\n",
    "\n",
    "ham = df[df['class']==0]\n",
    "spam = df[df['class']==1]\n",
    "\n",
    "# stop words per comment: histograms per class\n",
    "pyplot.hist(ham.stopwords_count, bins=50, alpha=0.5, label='ham', density=True)\n",
    "pyplot.hist(spam.stopwords_count, bins=50, alpha=0.5, label='spam', density=True)\n",
    "pyplot.xlim((-1,50))\n",
    "pyplot.ylim((0,0.4))\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words per comment: summary statistics per class\n",
    "print(ham.stopwords_count.describe())\n",
    "print()\n",
    "print(spam.stopwords_count.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Black Box Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by splitting our data into a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check of shapes\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we turn our textual data into numeric input, using sklearn's TF-IDF vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "\n",
    "#transform input data into numeric input\n",
    "x_train = vectorizer.fit_transform(train.content)\n",
    "y_train = train['class']\n",
    "x_test = vectorizer.transform(test.content)\n",
    "y_test = test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data: shape\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data: shape\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our classifiers. We will train a number of popular sklearn classifiers, using 10-fold cross-validation. We will then select the three best performing ones, to test Lime. \n",
    "\n",
    "Let's first import and store our classifiers in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('RFC', RandomForestClassifier(random_state=random_state))) \n",
    "models.append(('XGB', XGBClassifier(random_state=random_state)))\n",
    "models.append(('MLP', MLPClassifier(random_state=random_state, early_stopping= True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter tuning: defining the space\n",
    "parameter_space = {\n",
    "\n",
    "    'RFC': {'n_estimators' : [10, 50, 100, 200, 300],  \n",
    "             'max_depth' : [1, 3, 5, 10],  \n",
    "             'max_features' :['auto', 'sqrt']},\n",
    "    'XGB': {'n_estimators' : [10, 50, 100, 200, 300], \n",
    "            'max_depth' : [1, 3, 5, 10],  \n",
    "            'colsample_bytree' : [0.5, 1] , \n",
    "            'learning_rate': [0.001, 0.01, 0.1, 1]},\n",
    "    'MLP': { 'hidden_layer_sizes' :  [(10), (50), (100)], \n",
    "             'activation' :['tanh', 'relu'], \n",
    "             'learning_rate': ['constant','adaptive'], \n",
    "             'learning_rate_init': [1e-1, 1e-2, 1e-3]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# running the grid search \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "models_tuned = []\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'roc_auc'\n",
    "\n",
    "for name, model in models: \n",
    "    parameters = parameter_space[name]\n",
    "    clf = GridSearchCV(model, \n",
    "                        parameters,  \n",
    "                        cv = 10,\n",
    "                        #cv=5, \n",
    "                        verbose=1,\n",
    "                        scoring=scoring)\n",
    " \n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Best parameter set\n",
    "    print('Best parameters for {} found:\\n'.format(name), clf.best_params_)\n",
    "    print('Mean train score: {:.3f}'.format(clf.best_score_))\n",
    "    print('Standard Deviation {:.3f}'.format(clf.cv_results_['std_test_score'][clf.best_index_]))\n",
    "    print()\n",
    "    \n",
    "    # All results\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"{:.3f} (+/-{:.3f}) for {}\".format(mean, std * 2, params))\n",
    "    print()\n",
    "    \n",
    "    # Save results\n",
    "    models_tuned.append((name, clf.best_params_))\n",
    "    results.append(means)\n",
    "    names.append(name)\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the performance of the best models on test data\n",
    "\n",
    "import copy\n",
    "\n",
    "#reinitialize models with best parameters\n",
    "clf1 = copy.deepcopy(models[0][1]).set_params(**models_tuned[0][1])\n",
    "clf2 = copy.deepcopy(models[1][1]).set_params(**models_tuned[1][1])\n",
    "clf3 = copy.deepcopy(models[2][1]).set_params(**models_tuned[2][1])\n",
    "\n",
    "clf1.fit(x_train, y_train)\n",
    "clf2.fit(x_train, y_train)\n",
    "clf3.fit(x_train, y_train)\n",
    "\n",
    "pred1 = clf1.predict_proba(x_test)[:, 1]\n",
    "pred2 = clf2.predict_proba(x_test)[:, 1]\n",
    "pred3 = clf3.predict_proba(x_test)[:, 1]\n",
    "\n",
    "print (\"Roc_Auc Score RFC: \", sklearn.metrics.roc_auc_score(y_test, pred1))\n",
    "print (\"Roc_Auc Score XGB: \", sklearn.metrics.roc_auc_score(y_test, pred2))\n",
    "print (\"Roc_Auc Score MLP: \", sklearn.metrics.roc_auc_score(y_test, pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best RFC model: additional performance measures\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, clf1.predict(x_test))\n",
    "precision = precision_score(y_test, clf1.predict(x_test))\n",
    "recall = recall_score(y_test, clf1.predict(x_test), average='macro')\n",
    "\n",
    "# we print the measures on test data\n",
    "#print('Test AUC:', auc_res)\n",
    "print('Precision', precision)\n",
    "print('Recall', recall)\n",
    "print('Accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best XGB model: additional performance measures\n",
    "accuracy = accuracy_score(y_test, clf2.predict(x_test))\n",
    "precision = precision_score(y_test, clf2.predict(x_test))\n",
    "recall = recall_score(y_test, clf2.predict(x_test), average='macro')\n",
    "\n",
    "# we print the measures on test data\n",
    "#print('Test AUC:', auc_res)\n",
    "print('Precision', precision)\n",
    "print('Recall', recall)\n",
    "print('Accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best MLP model: additional performance measures\n",
    "accuracy = accuracy_score(y_test, clf3.predict(x_test))\n",
    "precision = precision_score(y_test, clf3.predict(x_test))\n",
    "recall = recall_score(y_test, clf3.predict(x_test), average='macro')\n",
    "\n",
    "# we print the measures on test data\n",
    "#print('Test AUC:', auc_res)\n",
    "print('Precision', precision)\n",
    "print('Recall', recall)\n",
    "print('Accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Explaining Predictions Using LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will analyze how LIME performs, when changing the following variables: sample size, number of features, tyoe of classifier, text length, and presence of stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define our class names, which we pass as argument to our explainer object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As sklearn uses vectorized input, but LIME assumes raw text input, we use sklearn's pipeline, which can apply a sequence of transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We continue by considering only the RFC and XGB (best) classifiers\n",
    "pipe_rfc = make_pipeline(vectorizer, clf1)\n",
    "pipe_xgb = make_pipeline(vectorizer, clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFC pipeline\n",
    "pipe_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LIME Output Generation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select three distinct instances/comments and we generate LIME explanations using the RFC and XGB (best) classifiers. Finally, we test different methods to visualize the LIME explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of the three instances/comments to be explained\n",
    "#----------------------------------------------------------\n",
    "#1.  (spam) idx = 4\n",
    "#2.  (spam) idx = 350 \n",
    "#3.   (ham) idx = 276 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the instance/comment\n",
    "idx = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen instance/comment\n",
    "test['content'].iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before computing LIME explanations: summary\n",
    "idx=idx\n",
    "pipeline = pipe_rfc\n",
    "\n",
    "print(\"Instance to be explained:\")\n",
    "#print(orig1)\n",
    "print(test['content'].iloc[idx])\n",
    "print(\"Actual class: \", test['class'].iloc[idx])\n",
    "print('Predicted probability class 0 =', pipeline.predict_proba([test['content'].iloc[idx]])[0,0])\n",
    "print('Predicted probability class 1 =', pipeline.predict_proba([test['content'].iloc[idx]])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class names for easier identification \n",
    "class_names = ['ham', 'spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default LimeTextExplainer()\n",
    "random_state=245\n",
    "class_names = ['ham', 'spam']\n",
    "explainer = LimeTextExplainer(class_names=class_names, \n",
    "                              feature_selection='none',\n",
    "                              random_state=random_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the explanation (change the number of features accordingle)\n",
    "explainer.random_state.seed(245)\n",
    "exp = explainer.explain_instance(test['content'].iloc[idx],\n",
    "                                 pipeline.predict_proba,\n",
    "                                 #num_features=10,\n",
    "                                 num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation as a list\n",
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation: intercept\n",
    "exp.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the explanation in a notebook\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the explanation\n",
    "%matplotlib inline\n",
    "fig = exp.as_pyplot_figure()\n",
    "#fig.savefig('INSERT PATH', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
